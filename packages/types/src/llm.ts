import { z } from "zod";

/**
 * LLM Provider Types
 */
export type LLMProvider = "openai" | "anthropic";

/**
 * LLM Request/Response Direction
 */
export type LLMDirection = "request" | "response";

/**
 * OpenAI Message Schema
 */
export const openaiMessageSchema = z.object({
  role: z.enum(["system", "user", "assistant", "tool"]),
  content: z.union([z.string(), z.null()]).optional(),
  name: z.string().optional(),
  tool_calls: z
    .array(
      z.object({
        id: z.string(),
        type: z.literal("function"),
        function: z.object({
          name: z.string(),
          arguments: z.string(), // JSON string
        }),
      }),
    )
    .optional(),
  tool_call_id: z.string().optional(),
});

/**
 * OpenAI Chat Completion Request Schema
 */
export const openaiChatCompletionRequestSchema = z.object({
  model: z.string(),
  messages: z.array(openaiMessageSchema),
  temperature: z.number().optional(),
  top_p: z.number().optional(),
  n: z.number().optional(),
  stream: z.boolean().optional(),
  stop: z.union([z.string(), z.array(z.string())]).optional(),
  max_tokens: z.number().optional(),
  presence_penalty: z.number().optional(),
  frequency_penalty: z.number().optional(),
  logit_bias: z.record(z.string(), z.number()).optional(),
  user: z.string().optional(),
  tools: z
    .array(
      z.object({
        type: z.literal("function"),
        function: z.object({
          name: z.string(),
          description: z.string().optional(),
          parameters: z.record(z.string(), z.unknown()).optional(),
        }),
      }),
    )
    .optional(),
  tool_choice: z
    .union([
      z.literal("none"),
      z.literal("auto"),
      z.object({
        type: z.literal("function"),
        function: z.object({ name: z.string() }),
      }),
    ])
    .optional(),
});

/**
 * OpenAI Chat Completion Response Schema
 */
export const openaiChatCompletionResponseSchema = z.object({
  id: z.string(),
  object: z.literal("chat.completion"),
  created: z.number(),
  model: z.string(),
  choices: z.array(
    z.object({
      index: z.number(),
      message: openaiMessageSchema,
      finish_reason: z
        .enum(["stop", "length", "tool_calls", "content_filter", "null"])
        .nullable(),
    }),
  ),
  usage: z
    .object({
      prompt_tokens: z.number(),
      completion_tokens: z.number(),
      total_tokens: z.number(),
    })
    .optional(),
});

/**
 * Anthropic Message Schema
 */
export const anthropicContentSchema = z.union([
  z.object({
    type: z.literal("text"),
    text: z.string(),
  }),
  z.object({
    type: z.literal("tool_use"),
    id: z.string(),
    name: z.string(),
    input: z.record(z.string(), z.unknown()),
  }),
  z.object({
    type: z.literal("tool_result"),
    tool_use_id: z.string(),
    content: z.string(),
  }),
]);

export const anthropicMessageSchema = z.object({
  role: z.enum(["user", "assistant"]),
  content: z.union([z.string(), z.array(anthropicContentSchema)]),
});

/**
 * Anthropic Messages Request Schema
 */
export const anthropicMessagesRequestSchema = z.object({
  model: z.string(),
  messages: z.array(anthropicMessageSchema),
  max_tokens: z.number(),
  temperature: z.number().optional(),
  top_p: z.number().optional(),
  top_k: z.number().optional(),
  stream: z.boolean().optional(),
  stop_sequences: z.array(z.string()).optional(),
  system: z.string().optional(),
  tools: z
    .array(
      z.object({
        name: z.string(),
        description: z.string().optional(),
        input_schema: z.record(z.string(), z.unknown()),
      }),
    )
    .optional(),
});

/**
 * Anthropic Messages Response Schema
 */
export const anthropicMessagesResponseSchema = z.object({
  id: z.string(),
  type: z.literal("message"),
  role: z.literal("assistant"),
  content: z.array(anthropicContentSchema),
  model: z.string(),
  stop_reason: z
    .enum(["end_turn", "max_tokens", "stop_sequence", "tool_use"])
    .nullable(),
  stop_sequence: z.string().nullable().optional(),
  usage: z.object({
    input_tokens: z.number(),
    output_tokens: z.number(),
  }),
});

/**
 * Generic LLM Request (normalized)
 */
export interface LLMRequest {
  id: string; // UUID generated by gateway
  traceId: string; // Correlation ID
  conversationId: string; // Groups multi-turn conversations
  timestamp: string; // ISO 8601
  provider: LLMProvider;
  model: string;
  direction: "request";

  // Original request body (provider-specific)
  requestBody: string; // JSON string

  // HTTP context
  userAgent?: string;
  clientIp?: string;
}

/**
 * Generic LLM Response (normalized)
 */
export interface LLMResponse {
  id: string; // Same as request ID
  traceId: string;
  conversationId: string;
  timestamp: string;
  provider: LLMProvider;
  model: string;
  direction: "response";

  // Response data
  responseBody: string; // JSON string
  finishReason?: string;
  streaming: boolean;

  // Metrics
  inputTokens?: number;
  outputTokens?: number;
  totalTokens?: number;
  durationMs: number;
  httpStatus: number;

  // Extracted tool calls for correlation
  toolCallsJson?: string; // JSON array of tool calls

  // HTTP context
  userAgent?: string;
  clientIp?: string;

  // Error information
  errorJson?: string;
}

/**
 * Conversation metadata
 */
export interface Conversation {
  conversationId: string;
  startTime: string;
  endTime?: string;
  provider: LLMProvider;
  model: string;
  totalRequests: number;
  totalTokens: number;
  totalDuration: number;
}

/**
 * Zod schemas for validation
 */
export const llmRequestSchema = z.object({
  id: z.string(),
  traceId: z.string(),
  conversationId: z.string(),
  timestamp: z.string(),
  provider: z.enum(["openai", "anthropic"]),
  model: z.string(),
  direction: z.literal("request"),
  requestBody: z.string(),
  userAgent: z.string().optional(),
  clientIp: z.string().optional(),
});

export const llmResponseSchema = z.object({
  id: z.string(),
  traceId: z.string(),
  conversationId: z.string(),
  timestamp: z.string(),
  provider: z.enum(["openai", "anthropic"]),
  model: z.string(),
  direction: z.literal("response"),
  responseBody: z.string(),
  finishReason: z.string().optional(),
  streaming: z.boolean(),
  inputTokens: z.number().optional(),
  outputTokens: z.number().optional(),
  totalTokens: z.number().optional(),
  durationMs: z.number(),
  httpStatus: z.number(),
  toolCallsJson: z.string().optional(),
  userAgent: z.string().optional(),
  clientIp: z.string().optional(),
  errorJson: z.string().optional(),
});

/**
 * Type exports
 */
export type OpenAIMessage = z.infer<typeof openaiMessageSchema>;
export type OpenAIChatCompletionRequest = z.infer<
  typeof openaiChatCompletionRequestSchema
>;
export type OpenAIChatCompletionResponse = z.infer<
  typeof openaiChatCompletionResponseSchema
>;
export type AnthropicContent = z.infer<typeof anthropicContentSchema>;
export type AnthropicMessage = z.infer<typeof anthropicMessageSchema>;
export type AnthropicMessagesRequest = z.infer<
  typeof anthropicMessagesRequestSchema
>;
export type AnthropicMessagesResponse = z.infer<
  typeof anthropicMessagesResponseSchema
>;
